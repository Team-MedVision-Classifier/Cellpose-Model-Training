{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7c7V4yEqDc_"
   },
   "source": [
    "# cellpose 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvyuR08OZfw4"
   },
   "source": [
    "We will first install cellpose 3.1, check the GPU is working, and mount google drive to get your models and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbqFni8kuFar"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlMnqge-lQ9s",
    "outputId": "2121e782-d3b5-463a-ace9-b09a6fa8097b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"opencv-python-headless<4.3\"\n",
    "!pip uninstall -y cellpose\n",
    "!pip install \"cellpose==3.1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2cBEO1PLuO7"
   },
   "source": [
    "`Check CUDA version and that GPU is working in cellpose and import other libraries.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt8hgC7rniP8"
   },
   "outputs": [],
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import core, utils, io, models, metrics\n",
    "from glob import glob\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from skimage import io\n",
    "\n",
    "\n",
    "use_GPU = core.use_gpu()\n",
    "yn = ['NO', 'YES']\n",
    "print(f'>>> GPU activated? {yn[use_GPU]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_files = natsorted(glob('train/*.tif'))\n",
    "train_seg = natsorted(glob('train/*.npy'))\n",
    "\n",
    "test_files = natsorted(glob('test/*.npy'))\n",
    "test_seg = natsorted(glob('test/*.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j4m33Fuw5vm"
   },
   "source": [
    "`what the training images look like + their labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "plt.figure(figsize=(12, 4), dpi=150)\n",
    "\n",
    "for k, f in enumerate(train_files):\n",
    "    # Load image\n",
    "    img = io.imread(f)\n",
    "\n",
    "    # Normalize image if needed\n",
    "    if img.dtype != np.uint8:\n",
    "        img = img.astype(np.float32)\n",
    "        img /= img.max() if img.max() > 0 else 1  # scale to [0, 1]\n",
    "\n",
    "    # Plot image\n",
    "    plt.subplot(2, len(train_files), k + 1)\n",
    "    img = np.vstack((img, np.zeros_like(img)[:1]))  # Optional: remove if not needed\n",
    "    img = img.transpose(1, 2, 0) if img.ndim == 3 else img\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Image\")\n",
    "\n",
    "    # Load and plot mask\n",
    "    plt.subplot(2, len(train_files), len(train_files) + k + 1)\n",
    "    seg_path = os.path.splitext(f)[0] + '_seg.npy'\n",
    "    seg = np.load(seg_path, allow_pickle=True).item()\n",
    "    masks = seg['masks'].squeeze()\n",
    "    plt.imshow(masks, cmap='nipy_spectral')  # color-coded mask\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Mask\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfE75htF0l84"
   },
   "source": [
    "# Train model on manual annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLdKNWQ4jxy5"
   },
   "source": [
    "## Training parameters\n",
    "\n",
    "<font size = 2> **Paths for training, predictions and results**\n",
    "\n",
    "\n",
    "<font size = 2>**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images).\n",
    "\n",
    "<font size = 2>**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
    "\n",
    "<font size = 2>**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "<font size = 2>**Training parameters**\n",
    "\n",
    "<font size = 2>**`number_of_epochs`:** Input how many epochs the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQI4aUxCjz3n",
    "outputId": "804d0459-b120-4298-9b4c-87a9ca26401c"
   },
   "outputs": [],
   "source": [
    "train_dir = \"train\" \n",
    "test_dir = \"test\" \n",
    "\n",
    "#Define where the patch file will be saved\n",
    "base = \"cellpose\"\n",
    "\n",
    "# model name and path\n",
    "from cellpose import models\n",
    "initial_model = \"nuclei\" # [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
    "model_name = \"ddq_model_1\"\n",
    "\n",
    "# other parameters for training.\n",
    "n_epochs =  100\n",
    "Channel_to_use_for_training = \"Red\" # [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
    "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# Advanced Parameters\n",
    "Use_Default_Advanced_Parameters = True\n",
    "learning_rate = 0.01 \n",
    "weight_decay = 0.0001 \n",
    "\n",
    "if (Use_Default_Advanced_Parameters):\n",
    "  print(\"Default advanced parameters enabled\")\n",
    "  learning_rate = 0.01\n",
    "  weight_decay = 0.0001\n",
    "\n",
    "#here we check that no model with the same name already exist, if so delete\n",
    "model_path = 'custom_models/'\n",
    "if os.path.exists(model_path+'/'+model_name):\n",
    "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
    "\n",
    "if len(test_dir) == 0:\n",
    "  test_dir = None\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_training == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_training == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_training == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_training == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "\n",
    "if Second_training_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_training_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_training_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_training_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "if initial_model=='scratch':\n",
    "  initial_model = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JRxBPmatrK7"
   },
   "source": [
    "## Train new model\n",
    "\n",
    "Using settings from form above, train model in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcYskYudMajM",
    "outputId": "ed4dce67-190e-4d52-9a70-bcbc76be4b65"
   },
   "outputs": [],
   "source": [
    "from cellpose import train\n",
    "\n",
    "# start logger (to see training across epochs)\n",
    "logger = io.logger_setup()\n",
    "\n",
    "# DEFINE CELLPOSE MODEL (without size model)\n",
    "model = models.CellposeModel(gpu=use_GPU, model_type=initial_model)\n",
    "\n",
    "# set channels\n",
    "channels = [chan, chan2]\n",
    "\n",
    "# get files\n",
    "output = io.load_train_test_data(train_dir, test_dir, mask_filter='_seg.npy')\n",
    "train_data, train_labels, _, test_data, test_labels, _ = output\n",
    "\n",
    "new_model_path, train_losses, test_losses = train.train_seg(model.net, train_data=train_data,\n",
    "                              train_labels=train_labels,\n",
    "                              test_data=test_data,\n",
    "                              test_labels=test_labels,\n",
    "                              channels=channels,\n",
    "                              save_path=model_path,\n",
    "                              n_epochs=n_epochs,\n",
    "                              learning_rate=learning_rate,\n",
    "                              weight_decay=weight_decay,\n",
    "                              SGD=True,\n",
    "                              nimg_per_epoch=8,\n",
    "                              model_name=model_name)\n",
    "\n",
    "# diameter of labels in training images\n",
    "diam_labels = model.net.diam_labels.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdH0j8-L6FuB"
   },
   "source": [
    "## Evaluate on test data (optional)\n",
    "\n",
    "If you have test data, check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0AGsH5p6K6S",
    "outputId": "3f67063a-2197-4ec2-8619-0b52c98fd0ee"
   },
   "outputs": [],
   "source": [
    "# get files (during training, test_data is transformed so we will load it again)\n",
    "output = io.load_train_test_data(test_dir, mask_filter='_seg.npy')\n",
    "test_data, test_labels = output[:2]\n",
    "\n",
    "# run model on test images\n",
    "masks = model.eval(test_data,\n",
    "                   channels=[chan, chan2],\n",
    "                   diameter=diam_labels)[0]\n",
    "\n",
    "# check performance using ground truth labels\n",
    "ap = metrics.average_precision(test_labels, masks)[0]\n",
    "print('')\n",
    "print(f'>>> average precision at iou threshold 0.5 = {ap[:,0].mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8tZ8uYR-IFW"
   },
   "source": [
    "plot masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=150)\n",
    "\n",
    "for k, im in enumerate(test_data):\n",
    "    # Copy image and normalize if needed\n",
    "    img = im.copy()\n",
    "    if img.dtype != np.uint8:\n",
    "        img = img.astype(np.float32)\n",
    "        img /= img.max() if img.max() > 0 else 1  # Normalize to [0,1]\n",
    "\n",
    "    # Optional padding (can remove if unnecessary)\n",
    "    img = np.vstack((img, np.zeros_like(img)[:1]))\n",
    "\n",
    "    # Transpose if in (C, H, W) format\n",
    "    if img.ndim == 3 and img.shape[0] <= 4:\n",
    "        img = img.transpose(1, 2, 0)\n",
    "\n",
    "    # Plot original image\n",
    "    plt.subplot(3, len(test_data), k + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if k == 0:\n",
    "        plt.title('Image')\n",
    "\n",
    "    # Plot predicted mask\n",
    "    plt.subplot(3, len(test_data), len(test_data) + k + 1)\n",
    "    plt.imshow(masks[k], cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "    if k == 0:\n",
    "        plt.title('Predicted Labels')\n",
    "\n",
    "    # Plot ground truth\n",
    "    plt.subplot(3, len(test_data), 2 * len(test_data) + k + 1)\n",
    "    plt.imshow(test_labels[k], cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "    if k == 0:\n",
    "        plt.title('True Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbVIZbNk5hgR"
   },
   "source": [
    "# Use custom model to segment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vDu4Ixjo588O"
   },
   "outputs": [],
   "source": [
    "model_path = \"custom_models/models/ddq_model_1\"\n",
    "image_dir = \"test\" \n",
    "\n",
    "# Channel Parameters:\n",
    "\n",
    "Channel_to_use_for_segmentation = \"Red\" # [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "Second_segmentation_channel= \"None\" # [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_segmentation == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_segmentation == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_segmentation == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "if Second_segmentation_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_segmentation_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_segmentation_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_segmentation_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "# Segmentation parameters:\n",
    "# diameter of cells (set to zero to use diameter from training set):\n",
    "diameter =  0 # {type:\"number\"}\n",
    "\n",
    "# threshold on flow error to accept a mask (set higher to get more cells, e.g. in range from (0.1, 3.0), OR set to 0.0 to turn off so no cells discarded):\n",
    "flow_threshold = 0.4 # {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
    "\n",
    "# threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
    "cellprob_threshold=0 # {type:\"slider\", min:-6, max:6, step:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN3rdsfMBc_8"
   },
   "source": [
    "## run custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCcbs722BYd0",
    "outputId": "b7de466b-4e7a-4585-b1d7-c282593b3fab"
   },
   "outputs": [],
   "source": [
    "# gets image files in dir (ignoring image files ending in _masks)\n",
    "# get all .tif files\n",
    "all_tifs = natsorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "\n",
    "# exclude any prediction masks\n",
    "files = [f for f in all_tifs if not f.endswith('_cp_masks.tif')]\n",
    "print(files)\n",
    "images = [io.imread(f) for f in files]\n",
    "\n",
    "# declare model\n",
    "model = models.CellposeModel(gpu=True,\n",
    "                             pretrained_model=model_path)\n",
    "\n",
    "# use model diameter if user diameter is 0\n",
    "diameter = model.diam_labels if diameter==0 else diameter\n",
    "\n",
    "# run model on test images\n",
    "masks, flows, styles = model.eval(images,\n",
    "                                  channels=[chan, chan2],\n",
    "                                  diameter=diameter,\n",
    "                                  flow_threshold=flow_threshold,\n",
    "                                  cellprob_threshold=cellprob_threshold\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwoUuuarC9V5"
   },
   "source": [
    "## save output masks to tiffs/pngs or txt files for imageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da-Rtx09DEZB"
   },
   "outputs": [],
   "source": [
    "io.save_masks(images,\n",
    "              masks,\n",
    "              flows,\n",
    "              files,\n",
    "              channels=[chan, chan2],\n",
    "              png=True, # save masks as PNGs and save example image\n",
    "              tif=True, # save masks as TIFFs\n",
    "              save_txt=True, # save txt outlines for ImageJ\n",
    "              save_flows=False, # save flows as TIFFs\n",
    "              save_outlines=True, # save outlines as TIFFs\n",
    "              save_mpl=False, # make matplotlib fig to view (WARNING: SLOW W/ LARGE IMAGES)\n",
    "              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "PiP9MWN4F3Sx",
    "outputId": "e70088bf-dbb2-4b49-a5df-620c2b253d68"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .npy visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cellpose import plot\n",
    "\n",
    "# Load TIFF image\n",
    "img_name = 'test/4Gy_53BP1_8Hr'\n",
    "img = imread(f'{img_name}.tif')\n",
    "\n",
    "# Handle (C, H, W) TIFFs\n",
    "if img.ndim == 3 and img.shape[0] <= 4:\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "\n",
    "# At this point img shape = (H, W, 2)\n",
    "H, W, C = img.shape\n",
    "assert C == 2, f\"Expected 2 channels, got {C}\"\n",
    "\n",
    "# Use first channel (e.g., blue = nuclei) as grayscale base for visualization\n",
    "base_img = img[..., 0]  # or img[..., 1] if you prefer green\n",
    "\n",
    "# Normalize if needed\n",
    "if base_img.max() > 255:\n",
    "    base_img = (base_img / base_img.max() * 255).astype(np.uint8)\n",
    "else:\n",
    "    base_img = base_img.astype(np.uint8)\n",
    "\n",
    "# Convert to RGB (grayscale stacked 3 times)\n",
    "img_rgb = np.stack([base_img] * 3, axis=-1)  # shape (H, W, 3)\n",
    "\n",
    "# Load Cellpose outputs\n",
    "seg_data = np.load(f'{img_name}_seg.npy', allow_pickle=True).item()\n",
    "masks = seg_data['masks']\n",
    "flows = seg_data['flows']\n",
    "# flow_y, flow_x = flows[:2]\n",
    "\n",
    "# Generate boundaries (outlines)\n",
    "outlines = find_boundaries(masks, mode='inner')  # Generates boolean mask of boundaries\n",
    "\n",
    "# Draw red outlines on the image\n",
    "img_outline = img_rgb.copy()\n",
    "img_outline[outlines] = np.array([255, 0, 0], dtype=np.uint8)  # Red color for outlines\n",
    "\n",
    "# Plotting the results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot the original base image\n",
    "axs[0].imshow(img_rgb)\n",
    "axs[0].set_title(\"Base Image (Channel 0)\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Plot the Cellpose masks\n",
    "axs[1].imshow(masks, cmap='nipy_spectral')\n",
    "axs[1].set_title(\"Cellpose Masks\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Plot the outlines on the original image\n",
    "imsave(f'{img_name}_outline.png',img_outline)\n",
    "axs[2].imshow(img_outline)\n",
    "axs[2].set_title(\"Outlines on Base Image\")\n",
    "axs[2].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CELL COUNTING COMPARISON: Ground Truth vs Generated vs Cellpose 3.1.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import io, models\n",
    "import os\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "print(\" COMPREHENSIVE CELL COUNTING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Count cells in ground truth (_seg.npy files)\n",
    "print(\"1Ô∏è GROUND TRUTH CELL COUNTS (_seg.npy files):\")\n",
    "gt_seg_files = natsorted(glob('test/*_seg.npy'))\n",
    "gt_counts = {}\n",
    "\n",
    "for seg_file in gt_seg_files:\n",
    "    img_name = os.path.basename(seg_file).replace('_seg.npy', '')\n",
    "    seg_data = np.load(seg_file, allow_pickle=True).item()\n",
    "    masks = seg_data['masks']\n",
    "    cell_count = len(np.unique(masks)) - 1  # Subtract 1 for background\n",
    "    gt_counts[img_name] = cell_count\n",
    "    print(f\"   {img_name}: {cell_count} cells\")\n",
    "\n",
    "# 2. Count cells in generated masks (cp_masks.tif files)\n",
    "print(f\"\\n2Ô∏è GENERATED MASK CELL COUNTS (cp_masks.tif files):\")\n",
    "cp_mask_files = natsorted(glob('test/*cp_masks.tif'))\n",
    "generated_counts = {}\n",
    "\n",
    "for mask_file in cp_mask_files:\n",
    "    img_name = os.path.basename(mask_file).replace('_cp_masks.tif', '')\n",
    "    masks = io.imread(mask_file)\n",
    "    cell_count = len(np.unique(masks)) - 1  # Subtract 1 for background\n",
    "    generated_counts[img_name] = cell_count\n",
    "    print(f\"   {img_name}: {cell_count} cells\")\n",
    "\n",
    "# 3. Run Cellpose 3.1.1 on original TIFF images and count cells\n",
    "print(f\"\\n3Ô∏è CELLPOSE 3.1.1 LIVE ANALYSIS (.tif images):\")\n",
    "tif_files = natsorted(glob('test/*.tif'))\n",
    "# Filter out cp_masks.tif files to only process original images\n",
    "tif_files = [f for f in tif_files if not f.endswith('cp_masks.tif')]\n",
    "cellpose_counts = {}\n",
    "\n",
    "# Load Cellpose nuclei model (avoid repeated downloads)\n",
    "if 'cellpose_model_cached' not in globals():\n",
    "    print(\"üîÑ Creating Cellpose model (one-time setup)...\")\n",
    "    cellpose_model_cached = models.Cellpose(gpu=True, model_type='nuclei')\n",
    "    print(\"‚úÖ Cellpose model created and cached\")\n",
    "else:\n",
    "    print(\"‚úÖ Using cached Cellpose model (no download needed)\")\n",
    "\n",
    "for tif_file in tif_files:\n",
    "    img_name = os.path.basename(tif_file).replace('.tif', '')\n",
    "    \n",
    "    # Load image\n",
    "    img = io.imread(tif_file)\n",
    "    \n",
    "    # Run Cellpose\n",
    "    masks, flows, styles, diams = cellpose_model_cached.eval(img, diameter=None, channels=[0,0])\n",
    "    cell_count = len(np.unique(masks)) - 1  # Subtract 1 for background\n",
    "    cellpose_counts[img_name] = cell_count\n",
    "    print(f\"   {img_name}: {cell_count} cells\")\n",
    "\n",
    "# 4. Create comparison summary\n",
    "print(f\"\\n CELL COUNT COMPARISON SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Image Name':<25} {'Ground Truth':<12} {'Generated':<12} {'Cellpose 3.1.1':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "all_images = set(gt_counts.keys()) | set(generated_counts.keys()) | set(cellpose_counts.keys())\n",
    "\n",
    "total_gt = 0\n",
    "total_generated = 0\n",
    "total_cellpose = 0\n",
    "\n",
    "for img_name in sorted(all_images):\n",
    "    gt = gt_counts.get(img_name, 'N/A')\n",
    "    gen = generated_counts.get(img_name, 'N/A')\n",
    "    cp = cellpose_counts.get(img_name, 'N/A')\n",
    "    \n",
    "    print(f\"{img_name:<25} {str(gt):<12} {str(gen):<12} {str(cp):<15}\")\n",
    "    \n",
    "    # Add to totals if numeric\n",
    "    if isinstance(gt, int): total_gt += gt\n",
    "    if isinstance(gen, int): total_generated += gen\n",
    "    if isinstance(cp, int): total_cellpose += cp\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTAL':<25} {total_gt:<12} {total_generated:<12} {total_cellpose:<15}\")\n",
    "\n",
    "\n",
    "# 6. Optional: Create a visualization\n",
    "if len(all_images) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x_pos = np.arange(len(all_images))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    gt_vals = [gt_counts.get(img, 0) for img in sorted(all_images)]\n",
    "    gen_vals = [generated_counts.get(img, 0) for img in sorted(all_images)]\n",
    "    cp_vals = [cellpose_counts.get(img, 0) for img in sorted(all_images)]\n",
    "    \n",
    "    # Create bars\n",
    "    ax.bar(x_pos - width, gt_vals, width, label='Ground Truth', alpha=0.8)\n",
    "    ax.bar(x_pos, gen_vals, width, label='Generated Masks', alpha=0.8)\n",
    "    ax.bar(x_pos + width, cp_vals, width, label='Cellpose 3.1.1', alpha=0.8)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Images')\n",
    "    ax.set_ylabel('Cell Count')\n",
    "    ax.set_title('Cell Count Comparison: Ground Truth vs Generated vs Cellpose 3.1.1')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([img[:15] + '...' if len(img) > 15 else img for img in sorted(all_images)], rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\" Bar chart visualization created above!\")\n",
    "\n",
    "print(\"\\n Cell counting analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
